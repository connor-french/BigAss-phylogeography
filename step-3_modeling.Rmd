---
title: "Modeling"
output: 
  html_document:
    toc: true
---

## Setup
Load packages
```{r setup, message=FALSE, warning=FALSE}
library(raster)
library(tidyverse)
library(tidymodels)
library(sf)
library(here)
library(wesanderson)
library(rnaturalearth)
library(patchwork)
library(blockCV)
library(tictoc)
library(vip)
library(furrr)
library(spdep)
library(ncf)
library(corrr)
library(rstan)
library(tidybayes)
```

### Map helpers
```{r map-helpers}
pal <- wes_palette("Zissou1", 100, type = "continuous")

# for assigning cells to continents. Islands are missed at coarser resolutions
world_base <- rnaturalearth::ne_countries(scale = "large", returnclass = "sf") %>%
  select(continent, name_long) %>% 
  st_transform(crs = "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs") 

# for mapping. this will be smaller so plotting is faster
world_base_map <- rnaturalearth::ne_countries(scale = "small", returnclass = "sf") %>%
  select(continent, name_long) %>% 
  st_transform(crs = "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs") 
```


## Data wrangling
Read in the data. Two main data sources- the genetic summary statistics and the environmental data. 
```{r data}
sumstats <- read_csv(here("output", "spreadsheets", "cell_high_3_10_sumstats.csv"))

rast_list_high <- list.files(here("data", "climate_agg"),
                        pattern = "high",
                        full.names = TRUE)

rasters_full_high <- raster::stack(rast_list_high)
crs(rasters_full_high) <- "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs"

glimpse(sumstats)
rasters_full_high
```



Extract raster values for each cell that has genetic summary data and join the data frames for a full data set. 
```{r full-data}
explanatory_df <- rasters_full_high[sumstats$cell] %>% 
  as_tibble() %>% 
  mutate(cell = sumstats$cell) 
  

full_df <- left_join(sumstats, explanatory_df, by = "cell") %>% 
  # convert soil classifications to a factor
  mutate(soil_high_hwsd = as.factor(soil_high_hwsd)) %>% 
  # arrange for plotting later
  arrange(cell)

glimpse(full_df)
```

Convert this to a `sf` polygon object for mapping and spatial cross validation. Also, extracting the continent for each polygon (largest overlap) to assess sampling imbalance.
```{r continents}
template_high <- raster(here("data", "templates", "template_high.tif"))

template_high[full_df$cell] <- full_df$avg_pi

full_sf <- rasterToPolygons(template_high) %>% 
  st_as_sf() %>% 
  bind_cols(full_df) %>% 
  select(-template_high) %>%
  # Adding continent column to the data frame.
  st_join(world_base["continent"], largest = TRUE)


glimpse(full_sf)
```

Visualizing temperature to make sure the conversion was successful. There are a couple tropical cells with low temperatures that don't make sense. I'll double-check later to see if these cells get filtered out through other means. If not, I'll figure out what to do with them.
```{r temp-vis}
ggplot() +
  geom_sf(data = full_sf, aes(fill = current_high_bio_1, color = current_high_bio_1)) +
  scale_fill_gradientn(colors = pal) +
  scale_color_gradientn(colors = pal) + 
  theme_minimal()
```

```{r}
ggplot() +
  geom_sf(data = full_sf, aes(fill = land_cover_high_evergreen_broadleaf_trees, color = land_cover_high_evergreen_broadleaf_trees)) +
  scale_fill_gradientn(colors = pal) +
  scale_color_gradientn(colors = pal) + 
  theme_minimal()
```


### Continent mapping
How many cells successfully mapped to a continent, and what is the sample size? Looks like 2 cells did not map correctly.
```{r continent-count}
full_sf %>% 
  count(continent)
```

Visualize which cells mapped correctly. All of the "Open Ocean" values are islands around Africa. Although these may politically be assigned a different continent (e.g. some islands around Madagascar are European), spatially they're near Africa, so I'm classifying them as African.  
```{r continent-map, cache=TRUE}
ggplot() + 
  geom_sf(data = world_base_map) +
  geom_sf(data = full_sf, aes(fill = continent, color = continent))
```

Converting the cells and taking another look. Looks fine, except for the NA. 
```{r continent-map-2, cache=TRUE}
full_sf <- full_sf %>% 
  mutate(continent = ifelse(str_detect(continent, "Seven"), "Africa", continent))

ggplot() + 
  geom_sf(data = world_base_map) +
  geom_sf(data = full_sf, aes(fill = continent, color = continent))
```


Let's see where the NA is. Looks like South America! 
```{r na-find, cache=TRUE}
full_sf %>% 
  mutate(na_cont = if_else(is.na(continent), "Missing", "Present")) %>% 
  ggplot() + 
  geom_sf(aes(fill = na_cont, color = na_cont))
```

Replacing the NA value with "South America". North America has the most representation, while South America has the least.
```{r continent-sampling}
full_complete <- full_sf %>% 
  mutate(continent = if_else(is.na(continent), "South America", continent))

count(full_complete, continent) %>% mutate(perc = n / sum(n)) %>% 
  as_tibble() %>% 
  select(continent, n, perc) %>%
  arrange(desc(perc)) %>% 
  knitr::kable()
```

### Predictor variables

Plant phylogenetic diversity has an excessive number of NAs (56). May need to throw out. Others have between 0 and 11 NAs. Need to see if these are for the same cells or not
```{r missing-data}
full_complete %>% 
  summarize_all(~sum(is.na(.))) %>% 
  pivot_longer(cols = !contains("geometry"), 
               names_to = "variable",
               values_to = "num_NA") %>% 
  filter(num_NA > 0)
```

It seems like there is some overlap with the missing data, but we're going to have to throw out 21 cells.
```{r}
nrow_filt <- full_complete %>% 
  select(-plant_high_plant_phylo) %>% 
  na.omit() %>% 
  nrow()

nrow(full_complete) - nrow_filt
```

#### Plant richness
Let's visualize where the plant richness diversity missing data is. It looks like the missing data is restricted to somewhat randomly distributed islands, including New Zealand. Let's explore a bit more to see if it would be worth diving into the data to fill in the missing values.
```{r}
full_complete %>% 
  mutate(plant_missing = 
           if_else(is.na(plant_high_plant_phylo), "Missing", "Present")) %>% 
  ggplot() + 
  geom_sf(aes(fill = plant_missing, color = plant_missing))
```

If there's no linear relationship with a Hill One or Average pi, then I'm just going to throw it out.

Plots don't look like much:
```{r}
p_hill <- ggplot(data = full_complete, aes(x = plant_high_plant_phylo, y = hill_1)) + 
  geom_point() + 
  geom_smooth()

p_pi <- ggplot(data = full_complete, aes(x = plant_high_plant_phylo, y = avg_pi)) + 
  geom_point() + 
  geom_smooth()


p_hill / p_pi
```

Linear models also show no real relationship (avg_pi is significant, but R2 is 0.02)
```{r}
summary(lm(hill_1 ~ plant_high_plant_phylo, data = full_complete))

summary(lm(avg_pi ~ plant_high_plant_phylo, data = full_complete))
```

**Decision**: Removing the plant richness as a predictor.  

#### Missing data
Let's see what the distribution of cells with an NA is, sans-plant richness. Most are islands, and there are a few from the Arctic.
```{r, cache=TRUE}
full_missing <- full_complete %>% select(-plant_high_plant_phylo)

full_missing <- full_missing[rowSums(is.na(full_missing)) > 0,]

ggplot() +
  geom_sf(data = world_base_map) +
  geom_sf(data = full_missing, aes(fill = continent, color = continent))
```

What is the sample size per cell? These are info-rich cells, but there are too many variables to get this info back from to where I don't think the effort is worth it. I'm going to remove them and call it good.
```{r}
full_missing %>% select(continent, num_otu, num_ind, num_order)
```


Remove the plant richness column and filter NAs. Also adding in latitude and longitude columns.
```{r}
full_filter <- full_complete %>% 
  select(-plant_high_plant_phylo) %>% 
  remove_missing() 

# get centroid coordinate for each cell
coords <- st_centroid(full_filter) %>% 
  st_coordinates()

full_filter <- full_filter %>% 
  mutate(longitude = coords[,1],
         latitude = coords[,2])

glimpse(full_filter)
```

Map the latitude to make sure I got the correct column.
```{r}
ggplot() +
  geom_sf(data = full_filter, aes(fill = latitude, color = latitude)) +
  scale_fill_gradientn(colors = pal) + 
  scale_color_gradientn(colors = pal)
```

### Data exploration

#### Read and filter
Read in genetic summary data for the least restrictive high resollution and medium resolution genetic filtering regimes. I can impose more restrictive filtering protocols from there.
```{r}
# least restrictive 3 individual filtering regimes
#high_3_df <- read_csv(here("output", "spreadsheets", "cell_high_3_10_sumstats.csv"))
med_3_df <- read_csv(here("output", "spreadsheets", "cell_medium_3_10_sumstats.csv"))
#low_3_df <- read_csv(here("output", "spreadsheets", "cell_low_3_10_sumstats.csv"))

# least restrictive 5 individual filtering regime
#high_5_df <- read_csv(here("output", "spreadsheets", "cell_high_5_10_sumstats.csv"))



```

Read in predictor variables
```{r}
# rast_list_high <- list.files(here("data", "climate_agg"),
#                         pattern = "high",
#                         full.names = TRUE)
# 
# rasters_full_high <- raster::stack(rast_list_high)
# crs(rasters_full_high) <- "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs"


rast_list_medium <- list.files(here("data", "climate_agg"),
                        pattern = "medium",
                        full.names = TRUE)

rasters_full_medium <- raster::stack(rast_list_medium)
crs(rasters_full_medium) <- "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs"

# rast_list_low <- list.files(here("data", "climate_agg"),
#                         pattern = "_low",
#                         full.names = TRUE)
# 
# rasters_full_low <- raster::stack(rast_list_low)
# crs(rasters_full_low) <- "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs"

```

Join the predictors with the genetic summaries.
```{r}
join_predictors <- function(gen_sumstats, pred_rasts, resolution){
  exp_df <- pred_rasts[gen_sumstats$cell] %>% 
    as_tibble() %>% 
    mutate(cell = gen_sumstats$cell)
  
  if (resolution == "high") {
    full_df <- left_join(gen_sumstats, exp_df, by = "cell") %>% 
      mutate(soil_high_hwsd = as.factor(soil_high_hwsd),
             resolution = "high") %>% 
      arrange(cell)
  } else if (resolution == "medium") {
    full_df <- left_join(gen_sumstats, exp_df, by = "cell") %>% 
      mutate(soil_medium_hwsd = as.factor(soil_medium_hwsd),
             resolution = "medium") %>% 
      arrange(cell) 
  } else {
    full_df <- left_join(gen_sumstats, exp_df, by = "cell") %>% 
      mutate(resolution = "low") %>% 
      arrange(cell) 
  }
  
  
  return(full_df)
}

# high_3_full <- join_predictors(high_3_df, rasters_full_high, "high")
# high_5_full <- join_predictors(high_5_df, rasters_full_high, "high")
med_3_full <- join_predictors(med_3_df, rasters_full_medium, "medium")
# low_3_full <- join_predictors(low_3_df, rasters_full_low, "low")


# nrow(high_3_full)
# nrow(high_5_full)
nrow(med_3_full)
# nrow(low_3_full)

```

Filter each dataset, removing the plant species richness and removing NAs.
```{r}
filter_dfs <- function(df, resolution) {
  plant_res <- paste0("plant_", resolution, "_plant_phylo")
  df_filtered <- df %>% 
    select(-c(plant_res)) %>% 
    remove_missing()
  return(df_filtered)
}

# high_3_filter <- filter_dfs(high_3_full, "high")
# high_5_filter <- filter_dfs(high_5_full, "high")
med_3_filter <- filter_dfs(med_3_full, "medium")
# low_3_filter <- filter_dfs(low_3_full, "low")


# nrow(high_3_filter)
# nrow(high_5_filter)
nrow(med_3_filter)
# nrow(low_3_filter)
```


#### PCA
Given that classes of variables are often highly correlated, I'm going to perform principle component analysis on sets of predictor variables and create new composite variables to investigate.  
Before performing the PCA on each data set, I'm going to create different data subsets, imposing stricter limits on the minimum number of OTUs per filtering regime.
I'm doing this for: current temperature, current precipitation, global habitat heterogeneity, terrain continuous average, terrain continuous standard deviation, and terrain categorical (percentages).
```{r}
all_dfs <- tibble(
  resolution = c(#paste(rep("high", 12, sep = ", ")), 
                 paste(rep("medium", 3, sep = ", "))#,
                 #paste(rep("low", 6, sep = ", "))
                 ),
  min_ind = c(# 3L, 3L, 3L, 3L, 3L, 3L, 
              #5L, 5L, 5L, 5L, 5L, 5L, 
             3L, 3L, 3L#,
              # 3L, 3L, 3L, 3L, 3L, 3L
              ),
  min_otu = c(100L, 150L, 200L),
  df = list(
    # high_3_filter,
    # high_3_filter %>% filter(num_otu >= 20),
    # high_3_filter %>% filter(num_otu >= 50),
    # high_3_filter %>% filter(num_otu >= 100),
    # high_3_filter %>% filter(num_otu >= 150),
    # high_3_filter %>% filter(num_otu >= 200),
    # high_5_filter,
    # high_5_filter %>% filter(num_otu >= 20),
    # high_5_filter %>% filter(num_otu >= 50),
    # high_5_filter %>% filter(num_otu >= 100),
    # high_5_filter %>% filter(num_otu >= 150),
    # high_5_filter %>% filter(num_otu >= 200),
    # med_3_filter,
    # med_3_filter %>% filter(num_otu >= 20),
    # med_3_filter %>% filter(num_otu >= 50),
    med_3_filter %>% filter(num_otu >= 100),
    med_3_filter %>% filter(num_otu >= 150),
    med_3_filter %>% filter(num_otu >= 200)
    # low_3_filter,
    # low_3_filter %>% filter(num_otu >= 20),
    # low_3_filter %>% filter(num_otu >= 50),
    # low_3_filter %>% filter(num_otu >= 100),
    # low_3_filter %>% filter(num_otu >= 150),
    # low_3_filter %>% filter(num_otu >= 200)
  )
) %>% 
  mutate(num_cells = map_int(df, nrow))

### function to conduct a pca for each variable set and each data frame and add the new PCA variables back to the original data frames

get_pc_scores <- function(df) {
  ### vectors of variables to perform pca on
  all_vars <- colnames(df)
  
  if (str_detect(all_vars[15], "high")) {
    # temperature bioclims
    temp_vars <- paste0("current_high_bio_", 1:11)
    # precipitation bioclims
    precip_vars <- paste0("current_high_bio_", 12:19)
  } else if (str_detect(all_vars[15], "medium")) {
    temp_vars <- paste0("current_medium_bio_", 1:11)
    # precipitation bioclims
    precip_vars <- paste0("current_medium_bio_", 12:19)
  } else if (str_detect(all_vars[15], "_low_")) {
    temp_vars <- paste0("current_low_bio_", 1:11)
    # precipitation bioclims
    precip_vars <- paste0("current_low_bio_", 12:19)
  }
  
  # global habitat heterogeneity
  ghh_vars <- all_vars[str_starts(all_vars, "ghh_")]
  
  # terrain continuous average
  terr_median_vars <- all_vars[str_ends(all_vars, "_median")]
  
  # terrain continuous standard deviation
  terr_sd_vars <- all_vars[str_ends(all_vars, "_sd")]
  
  # terrain categorical
  terr_cat_vars <- all_vars[str_detect(all_vars, "_geom")]
  
  # combine all into a single named list for looping
  predictor_list <- list(
    temp = temp_vars,
    precip = precip_vars,
    ghh = ghh_vars,
    terr_median = terr_median_vars,
    terr_sd = terr_sd_vars,
    terr_cat = terr_cat_vars
  )
  
  # function to perform the actual PCA
  perf_pca <- function(preds) {
    df_preds <- df %>% 
      select(all_of(preds))
    
    pca <- prcomp(df_preds, center = TRUE, scale. = TRUE)$x %>%
      as_tibble() %>%
      # retain the first two PCs
      select(paste0("PC", 1:2))
    
    return(pca)
  }
  
  # perform the PCA across variable sets
  pca_list <- invisible(map(predictor_list, perf_pca))
  
  names(pca_list) <- names(predictor_list)
  
  # assign prefixes to the PC column names so each variable set has distinct column names
  for (name in names(pca_list)) {
    colnames(pca_list[[name]]) <- paste0(name, "_", colnames(pca_list[[name]]))
  }

  out_df <- bind_cols(df, pca_list)

  return(out_df)
}

all_dfs_pca <- all_dfs %>% 
  mutate(df = map(df, get_pc_scores))

```


Normalize all variables so they are centered (mean of 0) and scaled (sd = 1) for linear regression.
```{r}
normalize_vars <- function(df_in) {
  norm_rec <- recipe(hill_1 ~ ., data = df_in) %>%
    step_normalize(all_numeric(), -cell, -num_otu, -num_ind, -num_order, -contains("_pi"), -contains("hill_"))
  
  df_trans <- norm_rec %>%
    prep(training = df_in) %>%
    juice(all_predictors()) %>%
    mutate(hill_1 = df_in$hill_1)
}

all_dfs_norm <- all_dfs_pca %>% 
  mutate(df_norm = map(df, normalize_vars),
         filter_regime = paste(resolution, min_ind, min_otu, sep = "_")) %>% 
  select(-df)


```

#### Map
I'm filtering out data sets that I already know I'm not going to use.
```{r}
all_dfs_norm <- all_dfs_norm %>% 
  filter(resolution == "medium",
         min_otu >= 100)
```


Convert to sf
```{r}
# functions to convert the data frames to sf objects
to_sf <- function(df) {
  if (df$resolution[1] == "high") {
    template <- raster(here("data", "templates", "template_high.tif"))
  } else if (df$resolution[1] == "medium") {
    template <- raster(here("data", "templates", "template_medium.tif"))
  } else {
    template <- raster(here("data", "templates", "template_low.tif"))
  }
  
  template[df$cell] <- df$num_order
  
  df_sf <- rasterToPolygons(template) %>% 
    st_as_sf() %>% 
    bind_cols(df) %>% 
    st_join(world_base["continent"], largest = TRUE)
  
  return(df_sf)
}


# convert the data frames to sf for plotting
all_dfs_sf <- all_dfs_norm %>% 
  mutate(df_sf = map(df_norm, to_sf))
```


Plot of spatial distribution for each filtering regime. Since there are so many maps, I am plotting two exemplars: Hill 1 for medium resolution with minimum 100 OTUs per cell and minimum 150 OTUs per cell. There's definitely spatial autocorrelation with Hill 1.
```{r}
#dfs_high <- filter(all_dfs_sf, resolution == "high")
dfs_medium <- filter(all_dfs_sf, resolution == "medium")
#dfs_low <- filter(all_dfs_sf, resolution == "low")
# 
# plot_maps_high <- function(df) {
#   ggplot() +
#     geom_sf(data = world_base_map, alpha = 0.5) +
#     geom_sf(
#       data = all_dfs_sf$df_sf[[1]],
#       fill = "grey",
#       color = "grey",
#       alpha = 0.7
#     ) +
#     geom_sf(data = df, aes(fill = hill_1, color = hill_1)) +
#     scale_fill_viridis_c() +
#     scale_color_viridis_c() +
#     theme_minimal()
# }

plot_maps_medium <- function(df) {
  ggplot() +
    geom_sf(data = world_base_map, alpha = 0.5) +
    geom_sf(
      data = all_dfs_sf$df_sf[[1]],
      fill = "transparent",
      color = "grey",
      alpha = 0.7
    ) +
    geom_sf(data = df, aes(fill = hill_1, color = hill_1)) +
    scale_fill_viridis_c() +
    scale_color_viridis_c() +
    labs(title = paste0("Number of cells: ", nrow(df))) +
    ggthemes::theme_map()
}

# plot_maps_low <- function(df) {
#   ggplot() +
#     geom_sf(data = world_base_map, alpha = 0.5) +
#     geom_sf(
#       data = all_dfs_sf$df_sf[[19]],
#       fill = "transparent",
#       color = "grey",
#       alpha = 0.7
#     ) +
#     geom_sf(data = df, aes(fill = hill_1, color = hill_1)) +
#     scale_fill_viridis_c() +
#     scale_color_viridis_c() +
#     labs(title = paste0("Number of cells: ", nrow(df))) +
#     ggthemes::theme_map()
# }


#maps_high <- map(dfs_high$df_sf, plot_maps_high)
maps_medium <- map(dfs_medium$df_sf, plot_maps_medium)
#maps_low <- map(dfs_low$df_sf, plot_maps_low)

maps_medium[[1]]

maps_medium[[2]]
```

#### Tropical-temperate divide
I'm adding a variable that indicates whether the cell is above or below the frost line (min temp of coldest month <= 0 degrees C). [White et al. 2019](https://doi.org/10.1038/s41467-019-10253-6) found that there is strong turnover in community composition across this line.  

First, read in and transform the data
```{r}
div_sf <- st_read(here("data", "climate_poly", "min_temp_binary.geojson")) %>% 
  st_transform(crs = "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs")

plot(div_sf)
```

Extract data for temperate-tropical divide.
```{r}
# hack to validate the multipolygon
div_sf_buf <- st_buffer(div_sf, dist = 0)

dfs_medium <- dfs_medium %>% 
  mutate(df_sf = map(df_sf, st_join, y = div_sf_buf, largest = TRUE))

```

```{r}
ggplot() +
  geom_sf(data=dfs_medium$df_sf[[1]], aes(fill = min_temp, color= min_temp))
```
```{r}
ggplot() +
  geom_boxplot(data=dfs_medium$df_sf[[1]], aes(y = hill_1, x = min_temp))
```

#### Species richness
SPECIES RICHNESS LOOKS TO JUST BE SAMPLING EFFORT

I'm adding insect species richness as a predictor for genetic diversity.

```{r}
insect_sp_richness <- st_read(here("data", "climate_poly", "insect_sp_richness.geojson"))

st_crs(insect_sp_richness) <- st_crs(dfs_medium$df_sf[[1]])

insect_sp_richness <- insect_sp_richness %>% 
  mutate(log_sp = log(n_species))
```

Looks like there sampling bias in the species richness estimate.
```{r}
ggplot() +
  geom_sf(data = world_base_map) +
  geom_sf(data = insect_sp_richness, aes(fill = log_sp, color = log_sp)) +
  scale_fill_viridis_c() +
  scale_color_viridis_c()
```

Add species richness to original data frame.
```{r}
dfs_medium <- dfs_medium %>% 
  mutate(df_sf = map(df_sf, st_join, y = insect_sp_richness, join = st_equals))
```


```{r}
ggplot() +
  geom_sf(data = dfs_medium$df_sf[[1]], aes(fill = log_sp, color = log_sp))
```



#### Spatial autocorrelation

Investigating spatial autocorrelation of response variables with Moran's I. First going to check out spatial autocorrelation of filtering regime we're likely to use (medium resolution, 150 OTU minimum). 

Plotting the neighborhood list to view the neighbors. The neighbors appear to be pretty tight. Going to want to view a correlogram at some point to see where spatial autocorrelation decays.
```{r}
sp_auto_df <- all_dfs_sf %>% 
  filter(filter_regime == "medium_3_150") %>% 
  pull(df_sf) %>% 
  pluck(1) %>% 
  select(hill_1) 

# create a neighborhood matrix (queen = TRUE means all neighbors, including diagonals, will be included)
sp_nb <- poly2nb(sp_auto_df, queen = TRUE)

# create a weights matrix. style = "W" means that the weights will be scaled from 0-1. This way we can compare across areas with different numbers of areas, which is true for this data set.
# also, ignoring cases with no neighbors. Errors would get thrown otherwise, and since we have islands with no neighbors, this would be a problem
sp_w <- nb2listw(sp_nb, style = "W", zero.policy = TRUE)



plot(sp_nb, coords = st_centroid(sp_auto_df) %>% st_coordinates())
```


Based on these Moran's I plots, it looks like there is spatial autocorrelation in Hill 1 under this strict neighborhood scheme. I plotted it two ways- including zeros and excluding them (turning them to NAs which is why the error appears). Excluding them shows clear spatial patterning;
```{r}
moran.plot(sp_auto_df$hill_1, sp_w,
           xlab = "Hill 1",
           ylab = "Neighbors Hill 1",
           zero.policy = TRUE)

moran.plot(sp_auto_df$hill_1, sp_w,
           xlab = "Hill 1",
           ylab = "Neighbors Hill 1",
           zero.policy = FALSE)


```

First, a quick Moran test to check for spatial autocorrelation. It's better to use the MCMC method since the regression-based method has some strict assumptions that our data likely violates. 

Hill 1 definitely has spatial autocorrelation under this neighborhood scheme.
```{r}
moran.mc(sp_auto_df$hill_1, sp_w, nsim = 1000, zero.policy = TRUE)
```


Now let's plot a spatial correlogram.
```{r}
sp_corr <- sp.correlogram(sp_nb, sp_auto_df$hill_1, order = 7, method = "I", randomisation = TRUE, zero.policy = TRUE)

print(sp_corr)
plot(sp_corr)
```


Moran's I touches zero at around 1,500 km.
```{r}
sp_coords <- st_centroid(sp_auto_df) %>% st_coordinates()
sp_moran_corr <- ncf::spline.correlog(x = sp_coords[,1], 
                                      y = sp_coords[,2], 
                                      z = sp_auto_df$hill_1,
                                      resamp = 100,
                                      latlon = FALSE,
                                      xmax = 3e6)

plot(sp_moran_corr)
```

I'm going to conduct moran's I tests from 500 km to 1500 km to see where spatial autocorrelation becomes insignificant.

I'm plotting the p-values vs distance. It looks like spatial autocorrelation reliably drops off at around 1,500 km.
```{r}
set.seed(998)
sp_correlog_test <- correlog(x = sp_coords[,1], 
                             y = sp_coords[,2], 
                             z = sp_auto_df$hill_1, 
                             increment = 10000, 
                             resamp = 1000, 
                             latlon = FALSE,
                             na.rm = TRUE, 
                             quiet = FALSE)

sp_correlog_df <- tibble(
  distance = sp_correlog_test$mean.of.class,
  pval = sp_correlog_test$p
  ) %>% 
  filter(distance >= 5e5, distance < 1.7e6)



ggplot(data = sp_correlog_df, aes(x = distance, y = pval)) +
  geom_point() +
  geom_hline(yintercept = 0.05, color = "red") + 
  theme_minimal()

```



#### Correlations
Exploring correlations among variables. Need to decide which to keep and which to throw out. I'm exploring the medium resolution, 150 km data set.  

Here are the variables I value:
**Climate**- Including all current bioclims in the correlation matrix. I am prioritizing the extremes (e.g. max temp of warmest month) and average (e.g. average annual temp). Katie says seasonality shouldn't have a huge effect since insects tend to aestivate/hibernate when conditions aren't ideal. However, since insects are ectotherms, extremes likely represent limits to insect tolerances and averages summarize the overall climate regime of the area.

**Habitat**- I have two datasets summarizing habitat variability: the dynamic habitat indices and habitat heterogeneity. For both, I am prioritizing measures of spatial heterogeneity, followed by average, followed by seasonality. The habitat heterogeneity measures only correspond with spatial heterogeneity.  

**Terrain**- I am limiting terrain to slope median and standard deviation and elevation median and standard deviation. The rest are derived stats that I couldn't justify using. Prioritizing sd since variation likely drives genetic diversity more than average

**Land Cover**- No land cover: highly spatially autocorrelated variables that only make sense to use in conjunction with each other.  

**Human**- only doing human modification since it's a specific measure of human environmental impact, rather than just human density  

**Stability**- including both temperature and precipitation stability. I doubt they're correlated. Keeping temperature if so, since precipitation is more difficult to model in past climates


```{r}
df_corr <- all_dfs_norm %>% 
  filter(filter_regime == "medium_3_150") %>% 
  pull(df_norm) %>% 
  pluck(1) %>% 
  select(hill_1,
         num_ind,
         num_otu,
         num_order,
         contains("_pi"),
    -contains("land_cover"),
         contains("elevation"),
         contains("slope"),
         contains("current"),
         contains("ghh"),
         contains("dhi"),
    contains("gHM"),
    contains("stability")) %>% 
  correlate()

```

I'm visualizing each set of variables separately first to make the correlations easier to interpret.

##### Climate

Keeping: 
BIO2 (mean diurnal range). It's uncorrelated with all other variables
BIO5, BIO6 (max temp warmest month, min temp coldest month). They're uncorrelated with each other and aren't strongly correlated with many other variables. BIO5 is strongly correlated with BIO1, so I'm throwing out BIO1 since extremes are higher priority.
BIO7 (temperature annual range). Represents extremes across the year.
BIO13, BIO14 (precipitation of wettest month, precipitation of driest month).
BIO15 (precipitation seasonality). Uncorrelated with BIO13 and BIO14. Also, precipitation seasonality varies regardless of temperate vs tropical regions, so maybe relevant on a global scale.

```{r}
# climate correlation matrix
remove_prefix <- function(x, pref = "current_medium_") {
  s <- str_remove_all(x, pref)
  return(s)
}

corr_clim <- df_corr %>% 
  filter(str_detect(rowname, "current")) %>% 
  select(rowname, contains("current")) %>% 
  rename_at(vars(contains("current")), remove_prefix) %>% 
  mutate(rowname = str_remove_all(rowname, "current_medium_"))

corrr::rplot(corr_clim)
```

##### Habitat
I'm selecting cumulative DHI, var DHI, variance GHH, and standard deviation GHH, as these are uncorrelated with each other. While minimum DHI could be considered an "extreme", it does not reflect physiological limits, so since it was correlated with both cum DHI and var DHI, while those two variables are not correlated with each other, I'm going to select them since they likely explain more independent information than min DHI. There were many options for GHH, but variance was correlated with the fewest variables and standard deviation is uncorrelated with variance and also is a recognizable measure of variation.

```{r}

corr_hab <- df_corr %>% 
  filter(str_detect(rowname, "ghh|dhi"),
         !str_detect(rowname, "PC")) %>% 
  select(rowname, contains("ghh"), contains("dhi"), -contains("PC")) %>% 
  rename_at(vars(contains("ghh")), ~remove_prefix(.x, "_medium")) %>%
  rename_at(vars(contains("dhi")), ~remove_prefix(.x, "_medium")) %>%
  mutate(rowname = str_remove_all(rowname, "_medium"))


corrr::rplot(corr_hab)
```


##### Terrain
Retaining elevation median and standard deviation. Both correlate with slope median and sd. Elevation likely matters more than slope.
```{r}
corr_terr <- df_corr %>% 
  filter(str_detect(rowname, "elevation|slope"),
         !str_detect(rowname, "geom")) %>% 
  select(rowname, contains("elevation"), contains("slope"), -contains("geom")) 

corrr::rplot(corr_terr)
```

##### Refined correlation matrix
Now I'm going to look at a correlation matrix of this reduced set of variables. Priority is climate > habitat > human > terrain for selection. Climate variables most likely translate to a larger scale the best, and terrain variables are likely the most indirect predictors of genetic diversity. Habitat is probably most directly relevant, but the noise in the variables is likely very high at the coarse scales we're looking at. They're mostly measured at a fine scale.


Bioclims 6 and 7 are both correlated with bio 13, and bioclim 6 is correlated with DHI var, so I'm removing them. 
```{r}
climate_vars <- c("bio_2", "bio_5", "bio_6", "bio_7", "bio_13", "bio_14", "bio_15")
habitat_vars <- c("_cum", "_var", "_variance", "std_dev")
terrain_vars <- c("elevation_median", "elevation_sd")
stability_vars <- c("stability")
human_vars <- c("gHM")

all_vars <- c(climate_vars, 
              habitat_vars, 
              terrain_vars, 
              stability_vars, 
              human_vars) %>% 
  paste(collapse = "|")

all_vars_full <- df_corr$rowname[str_detect(df_corr$rowname, all_vars)]

corr_reduced <- df_corr %>% 
  select(rowname, any_of(all_vars_full), -contains("coef_of_var"), contains("gHM"), contains("stability")) %>% 
  filter(str_detect(rowname, all_vars), !str_detect(rowname, "coef_of_var"))
```

Get final correlation matrix. Everything looks good!
```{r}
corr_final <- corr_reduced %>% 
  filter(!str_detect(rowname, "bio_6|bio_7")) %>% 
  select(-contains("bio_6"), -contains("bio_7"))

corr_final_vec <- corr_final$rowname

corrr::rplot(corr_final)
```

#### Linear models

Read in data so I don't have to re-run everything previously. Filtering out the point with a Hill number ~0.344 because it's on a tiny island that many of the terrestrial environmental variables don't capture.
```{r}
df_150 <- st_read(here("output", "spreadsheets", "medium_150.geojson"),
                  crs = "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs") %>% 
  filter(hill_1 > 0.35)
corr_final_vec <- read_csv(here("output", "spreadsheets", "keep_nocorr_vars.csv")) %>% 
  pull(1)
```



##### Multiple regression- spatial autocorrelation

Fitting an OLS multiple regression to assess the extent of spatial autocorrelation in the residuals and guide the use of a conditional autoregression.

Fit a multiple regression with the final set of variables.
```{r}
lm_df <- df_150 %>% 
  as_tibble() %>% 
  select(hill_1, any_of(corr_final_vec), -geometry)
  

hill_model_lm <- lm(hill_1 ~ ., data = lm_df)

hill_resid_lm <- hill_model_lm %>% 
  augment() %>% 
  select(.resid)

resid_sf <- df_150 %>% 
  select(hill_1) %>% 
  bind_cols(hill_resid_lm)

ggplot() +
  geom_sf(data = resid_sf, aes(fill = .resid, color = .resid)) +
  scale_fill_viridis_c() +
  scale_color_viridis_c()
```

The residuals don't look like they're spatially autocorrelated, but let's see. Looks to be lower, but let's check out significance.
```{r}
sp_coords_resid <- st_centroid(resid_sf) %>% st_coordinates()
sp_moran_corr <- ncf::spline.correlog(x = sp_coords_resid[,1], 
                                      y = sp_coords_resid[,2], 
                                      z = resid_sf$.resid,
                                      resamp = 100,
                                      latlon = FALSE,
                                      xmax = 3e6)

plot(sp_moran_corr)
```

Looks like there is significant spatial autocorrelation out to about 1000 km for the full model. 
```{r}
set.seed(928)
resid_correlog_test <- correlog(x = sp_coords_resid[,1], 
                             y = sp_coords_resid[,2], 
                             z = resid_sf$.resid, 
                             increment = 10000, 
                             resamp = 1000, 
                             latlon = FALSE,
                             na.rm = TRUE, 
                             quiet = FALSE)

resid_correlog_df <- tibble(
  distance = resid_correlog_test$mean.of.class,
  pval = resid_correlog_test$p
  ) %>% 
  filter(distance < 1.7e6)



ggplot(data = resid_correlog_df, aes(x = distance, y = pval)) +
  geom_point() +
  geom_hline(yintercept = 0.05, color = "red") + 
  labs(title = paste0("SAC is no longer present at ", round(resid_correlog_test$x.intercept / 1000, 3), " km")) +
  theme_minimal()

```
##### Random Fields GLMM
```{r}
library(glmmfields)
options(mc.cores = parallel::detectCores())
```


Explore priors
```{r}
sigmas <- c(0.2, 0.5, 1, 3, 5, 10)
dfs <- c(3, 5, 10, 100, 1000)

tdist_dfs <- map_dfc(dfs, ~rstudent_t(df = .x, n = 1000, mu = 0, sigma = 2))

colnames(tdist_dfs) <- paste0("df_", dfs)

tdist_dfs <- tdist_dfs %>% 
  pivot_longer(cols = everything(), names_to = "df", values_to = "values")

ggplot(data = tdist_dfs %>% filter(df == "df_100"), aes(x = values, color = df)) +
  geom_density(adjust = 3) +
  scale_fill_viridis_d()
```

Get a data frame with the centroid coordinates of each cell for the regression. I need to scale them down to ~1-10 for the sigma-theta parameter estimation (decay of spatial autocorrelation with distance), which depends on the distance between points
```{r}
df_spatial <- df_150 %>% 
  bind_cols(df_150 %>% 
              st_centroid() %>% 
              st_coordinates() %>% 
              as_tibble()) %>% 
  as_tibble() %>% 
  select(-geometry) %>% 
  mutate(lon_scaled = X * 0.000001,
         lat_scaled = Y * 0.000001)
```



###### GDE Full model
```{r}
model_rf <- glmmfields(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    ghh_medium_std_dev +
    human_medium_gHM +
    stability_precip_medium +
    stability_temp_medium,
  data = df_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 25,
  iter = 7000,
  save_log_lik = TRUE,
  chains = 4,
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 100,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(100, 0, 1),
  prior_gp_theta = half_t(100, 0, 5),
  prior_gp_sigma = half_t(100, 0, 1),
  control = list(adapt_delta = 0.9999,
                 max_treedepth = 15),
  seed = 1457
)
```





```{r}
model_rf
```


###### GDE current new stability
```{r}
new_stab_files <- list.files("~/Downloads/data/past_climate", full.names = TRUE)

new_stab_rasters <- stack(new_stab_files)

new_stab_sf <- new_stab_rasters %>%  
  projectRaster(template_medium_rast) %>% 
  rasterToPolygons() %>% 
  st_as_sf()

# take the median of all overlapping cells with each of the medium resolution cells
new_stab_df <- st_join(df_150, 
                         new_stab_sf,
                         largest = TRUE)

new_stab_spatial <- new_stab_df %>% 
  bind_cols(new_stab_df %>% 
              st_centroid() %>% 
              st_coordinates() %>% 
              as_tibble()) %>% 
  as_tibble() %>% 
  mutate(temp_trend = normalize(GlobalExtreme_tsTrendExt),
         temp_var = normalize(GlobalExtreme_tsVarExt),
         precip_trend = normalize(GlobalExtreme_prTrendExt),
         precip_var = normalize(GlobalExtreme_prVarExt)) %>% 
  select(-geometry) %>% 
  mutate(lon_scaled = X * 0.000001,
         lat_scaled = Y * 0.000001)

model_cns <- glmmfields(
  hill_1 ~
    current_medium_bio_1 +
    #current_medium_bio_12 +
    temp_trend,
    #temp_var + 
    #precip_var +
    #precip_trend,
  data = new_stab_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 20,
  iter = 3000,
  chains = 2,
  save_log_lik = TRUE,
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 100,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(100, 0, 1),
  prior_gp_theta = half_t(100, 0, 5),
  prior_gp_sigma = half_t(100, 0, 1),
  control = list(adapt_delta = 0.9999,
                 max_treedepth = 15),
  seed = 1375
)
```

```{r}
post_draws_cns <- posterior_predict(model_cns, iter = 100) %>% 
  t()
colnames(post_draws_cns) <- paste0("draw_", 1:100)

post_df_cns <- post_draws_cns %>% 
  as_tibble() %>% 
  mutate(observed = model_cns$data$hill_1) %>% 
  pivot_longer(cols = everything(),
               names_to = "draw",
               values_to = "hill_1_post") %>% 
  mutate(post_samples = if_else(draw == "observed", "observed", "posterior"))

ggplot(data = post_df_cns, aes(x = hill_1_post, 
                           group = draw,
                           color = post_samples)) +
  stat_density(geom = "line", alpha = 0.6, position = "identity") +
  scale_color_manual(values = c("darkgreen", "darkgrey")) +
  theme_minimal()


```
```{r}
cns_post_sum <- tidy(model_cns, conf.int = TRUE, conf.method = "HPDinterval") 
  
cns_post_beta <- cns_post_sum %>% 
  filter(str_detect(term, "B"),
         !str_detect(term, "[1]")) %>% 
  mutate(significance = if_else(
    conf.low <= 0 & conf.high >= 0, 0.6, 1.0
  ))

ggplot(data = cns_post_beta, aes(x = term, alpha = significance)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  scale_alpha(guide = "none") +
  
  coord_flip() +
  theme_minimal()
```


###### GDE Current + Stability

```{r}

model_cs <- glmmfields(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    stability_precip_medium +
    stability_temp_medium,
  data = df_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 25,
  iter = 7000,
  chains = 4,
  save_log_lik = TRUE,
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 100,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(100, 0, 1),
  prior_gp_theta = half_t(100, 0, 5),
  prior_gp_sigma = half_t(100, 0, 1),
  control = list(adapt_delta = 0.9999,
                 max_treedepth = 15),
  seed = 127
)
```

```{r}
model_cs
```

Posterior density. 100 draws from the posterior prediction distribution overlaid with the observed Hill 1.
```{r}
post_draws <- posterior_predict(model_cs, iter = 100) %>% 
  t()
colnames(post_draws) <- paste0("draw_", 1:100)

post_df <- post_draws %>% 
  as_tibble() %>% 
  mutate(observed = model_cs$data$hill_1) %>% 
  pivot_longer(cols = everything(),
               names_to = "draw",
               values_to = "hill_1_post") %>% 
  mutate(post_samples = if_else(draw == "observed", "observed", "posterior"))

ggplot(data = post_df, aes(x = hill_1_post, 
                           group = draw,
                           color = post_samples)) +
  stat_density(geom = "line", alpha = 0.6, position = "identity") +
  scale_color_manual(values = c("darkgreen", "darkgrey")) +
  theme_minimal()

```


```{r}
cs_post_sum <- tidy(model_cs, conf.int = TRUE, conf.method = "HPDinterval") 
  
cs_post_beta <- cs_post_sum %>% 
  filter(str_detect(term, "B"),
         !str_detect(term, "[1]")) %>% 
  mutate(significance = if_else(
    conf.low <= 0 & conf.high >= 0, 0.6, 1.0
  ))

ggplot(data = cs_post_beta, aes(x = term, alpha = significance)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  scale_alpha(guide = "none") +
  
  coord_flip() +
  theme_minimal()
```

Beta posteriors
```{r}
post_cs <- rstan::extract(model_cs$model, permute = TRUE)

post_beta <- post_cs$B

colnames(post_beta) <- c(
  "intercept",
  "Prec. Wettest Month",
    "Prec. Driest Month",
    "Prec. Seasonality",
    "Temp. Diurnal Range",
    "Max. Temp Warmest Month",
    "Precip. Stability",
    "Temp. Stability"
)
post_beta_df <- as_tibble(post_beta)

post_beta_long <- post_beta_df %>% 
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "posterior") %>% 
  mutate(significance = case_when(
    variable == "Max. Temp Warmest Month" ~ 1.0,
    #variable == "Temp. Diurnal Range" ~ 1.0,
    variable == "Temp. Stability" ~ 1.0,
    TRUE ~ 0.0
  ))



post_beta_plot <- post_beta_long %>% 
  filter(variable != "intercept") %>% 
  ggplot(aes(x = posterior, y = variable, alpha = significance)) +
  ggridges::geom_density_ridges() +
  scale_alpha(guide = "none") +
  labs(x = "Effect size", y = "Predictor") +
  geom_vline(xintercept = 0, color = "darkgreen") + 
  theme_minimal() +
  theme(axis.title.y = element_blank())

post_beta_plot
```


###### GDE Temp
```{r}
temp_trend_spatial <- temp_trend_df %>% 
  bind_cols(temp_trend_df %>% 
              st_centroid() %>% 
              st_coordinates() %>% 
              as_tibble()) %>% 
  as_tibble() %>% 
  mutate(GlobalExtreme_tsTrendExt = normalize(GlobalExtreme_tsTrendExt)) %>% 
  select(-geometry) %>% 
  mutate(lon_scaled = X * 0.000001,
         lat_scaled = Y * 0.000001)
  

model_t <- glmmfields(
  hill_1 ~
    current_medium_bio_2 +
    current_medium_bio_5 +
    GlobalExtreme_tsTrendExt,
  data = temp_trend_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 25,
  iter = 3000,
  chains = 4,
  save_log_lik = TRUE,
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 100,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(100, 0, 1),
  prior_gp_theta = half_t(100, 0, 5),
  prior_gp_sigma = half_t(100, 0, 1),
  control = list(adapt_delta = 0.9999,
                 max_treedepth = 15),
  seed = 53699
)
```

```{r}
model_t
```

Posterior density. 100 draws from the posterior prediction distribution overlaid with the observed Hill 1.
```{r}
post_draws <- posterior_predict(model_t, iter = 100) %>% 
  t()
colnames(post_draws) <- paste0("draw_", 1:100)

post_df <- post_draws %>% 
  as_tibble() %>% 
  mutate(observed = model_t$data$hill_1) %>% 
  pivot_longer(cols = everything(),
               names_to = "draw",
               values_to = "hill_1_post") %>% 
  mutate(post_samples = if_else(draw == "observed", "observed", "posterior"))

ggplot(data = post_df, aes(x = hill_1_post, 
                           group = draw,
                           color = post_samples)) +
  stat_density(geom = "line", alpha = 0.6, position = "identity") +
  scale_color_manual(values = c("darkgreen", "darkgrey")) +
  theme_minimal()

```


```{r}

t_post_sum <- tidy(model_t, conf.int = TRUE, conf.method = "HPDinterval") 
  
t_post_beta <- t_post_sum %>% 
  filter(str_detect(term, "B"),
         !str_detect(term, "[1]")) %>% 
  mutate(significance = if_else(
    conf.low <= 0 & conf.high >= 0, 0.6, 1.0
  ))

ggplot(data = t_post_beta, aes(x = term, alpha = significance)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  scale_alpha(guide = "none") +
  
  coord_flip() +
  theme_minimal()
```

###### GDE Posterior plots
Posterior density. 100 draws from the posterior prediction distribution overlaid with the observed Hill 1.
```{r}
post_draws <- posterior_predict(model_t, iter = 100) %>% 
  t()
colnames(post_draws) <- paste0("draw_", 1:100)

post_df <- post_draws %>% 
  as_tibble() %>% 
  mutate(observed = model_t$data$hill_1) %>% 
  pivot_longer(cols = everything(),
               names_to = "draw",
               values_to = "hill_1_post") %>% 
  mutate(post_samples = if_else(draw == "observed", "observed", "posterior"))

ggplot(data = post_df, aes(x = hill_1_post, 
                           group = draw,
                           color = post_samples)) +
  stat_density(geom = "line", alpha = 0.6, position = "identity") +
  scale_color_manual(values = c("darkgreen", "darkgrey")) +
  theme_minimal()

```


B2 = current_medium_bio_13, B3 = current_medium_bio_14, B4 = current_medium_bio_15, B5 = current_medium_bio_2, B6 = current_medium_bio_5, B7 = ghh_medium_std_dev, B8 = stability_temp_medium

```{r}

rf_post_sum <- tidy(model_rf, conf.int = TRUE, conf.method = "HPDinterval") 
  
rf_post_beta <- rf_post_sum %>% 
  filter(str_detect(term, "B"),
         !str_detect(term, "[1]")) %>% 
  mutate(significance = if_else(
    conf.low <= 0 & conf.high >= 0, 0.6, 1.0
  ))

ggplot(data = rf_post_beta, aes(x = term, alpha = significance)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  scale_alpha(guide = "none") +
  
  coord_flip() +
  theme_minimal()
```

Beta posteriors
```{r}
post_rf <- rstan::extract(model_rf$model, permute = TRUE)

post_beta <- post_rf$B

colnames(post_beta) <- c(
  "intercept",
  "Prec. Wettest Month",
    "Prec. Driest Month",
    "Prec. Seasonality",
    "Temp. Diurnal Range",
    "Max. Temp Warmest Month",
    "Habitat Stand. Dev.",
    "Temp. Stability"
)

post_beta_df <- as_tibble(post_beta)

post_beta_long <- post_beta_df %>% 
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "posterior") %>% 
  mutate(significance = case_when(
    variable == "Max. Temp Warmest Month" ~ 1.0,
    variable == "Temp. Diurnal Range" ~ 1.0,
    variable == "Temp. Stability" ~ 1.0,
    TRUE ~ 0.0
  ))



post_beta_plot <- post_beta_long %>% 
  filter(variable != "intercept") %>% 
  ggplot(aes(x = posterior, y = variable, alpha = significance)) +
  ggridges::geom_density_ridges() +
  scale_alpha(guide = "none") +
  labs(x = "Effect size", y = "Predictor") +
  geom_vline(xintercept = 0, color = "darkgreen") + 
  theme_minimal() +
  theme(axis.title.y = element_blank())

post_beta_plot
```

Intercept posterior
```{r}
post_beta_long %>% 
  filter(variable == "intercept") %>% 
  ggplot(aes(x = posterior)) +
  labs(y = "") +
  geom_density(fill = "grey") +
  labs(x = "Intercept") +
  theme_minimal()
```


```{r}
rf_linpred <- posterior_linpred(model_rf, iter = 100)

rf_linpred_draws %>% 
  ggplot(aes(x = current_medium_bio_5, y = hill_1)) +
  #stat_lineribbon(aes(y = .value)) +
  geom_line(aes(y = .value, group = .draw), alpha = .1) +
  geom_point(data = model_rf$data) +
  scale_fill_brewer(palette = "Greys") 

```

Maximum temperature of the warmest month
```{r}
post_beta_sample <- sample_n(post_beta_df, 1000)

bio_5_plot <- ggplot(data = model_rf$data, 
             aes(x = current_medium_bio_5, y = hill_1)) +
  geom_point() +
  geom_abline(data = post_beta_sample, 
              aes(intercept = intercept, 
                  slope = `Max. Temp Warmest Month`), 
              alpha = 0.05,
              color = "darkgrey") +
  geom_abline(intercept = rf_post_sum %>% 
                filter(term == "B[1]") %>% 
                pull(estimate),
              slope = rf_post_sum %>% 
                filter(term == "B[6]") %>% 
                pull(estimate),
              color = "darkgreen") +
  labs(x = "Maximum temperature of the warmest month",
       y = "Genetic evenness") +
  theme_minimal()

bio_5_plot
```

Temperature mean diurnal range
```{r}
bio_2_plot <- ggplot(data = model_rf$data, 
             aes(x = current_medium_bio_2, y = hill_1)) +
  geom_point() +
  geom_abline(data = post_beta_sample, 
              aes(intercept = intercept, 
                  slope = `Temp. Diurnal Range`), 
              alpha = 0.05,
              color = "darkgrey") +
  geom_abline(intercept = rf_post_sum %>% 
                filter(term == "B[1]") %>% 
                pull(estimate),
              slope = rf_post_sum %>% 
                filter(term == "B[5]") %>% 
                pull(estimate),
              color = "darkgreen") +
  labs(x = "Temperature mean diurnal range",
       y = "Genetic evenness") +
  theme_minimal()

bio_2_plot
```

Temperature stability
```{r}
temp_stability_plot <- ggplot(data = model_rf$data, 
             aes(x = stability_temp_medium, y = hill_1)) +
  geom_point() +
  geom_abline(data = post_beta_sample, 
              aes(intercept = intercept, 
                  slope = `Temp. Stability`), 
              alpha = 0.05,
              color = "darkgrey") +
  geom_abline(intercept = rf_post_sum %>% 
                filter(term == "B[1]") %>% 
                pull(estimate),
              slope = rf_post_sum %>% 
                filter(term == "B[8]") %>% 
                pull(estimate),
              color = "darkgreen") +
  labs(x = "Temperature stability",
       y = "Genetic evenness") +
  theme_minimal()

temp_stability_plot
```

Boxplot of freezing line division
```{r}
freeze_plot <- model_rf$data %>% 
  mutate(freezeline = if_else(min_temp == "temperate", "Below", "Above")) %>% 
  ggplot(aes(x = freezeline, y = hill_1)) +
  geom_boxplot() +
  geom_jitter(width = 0.2, alpha = 0.6, color = "darkgreen") +
  labs(x = "Freezeline position",
       y = "Genetic evenness") +
  theme_minimal()

freeze_plot
```

```{r}
(freeze_plot + post_beta_plot) / (bio_5_plot + bio_2_plot + temp_stability_plot) + plot_annotation(tag_levels = 'A')
```

Global map of genetic diversity
```{r}
predictors <- colnames(model_rf$X)[-1]
global_predictors <- as.data.frame(rasters_full_medium,
                                   xy = TRUE,
                                   centroids = TRUE,
                                   na.rm = TRUE) %>% 
  select(predictors, x, y)

global_predictors <- global_predictors %>% 
  mutate(cell = rownames(global_predictors),
         lon_scaled = x * 0.000001,
         lat_scaled = y * 0.000001)


normalize <- function(x) {
  a <- x - mean(x)
  b <- sd(x)
  c <- a / b
  return(c)
}

predictors_norm <- global_predictors %>% 
  mutate_at(predictors, normalize) 

global_prediction <- predict(model_rf, 
                             predictors_norm, 
                             estimate_method = "median",
                             conf_level = 0.95,
                             interval = "confidence")

map_df <- bind_cols(predictors_norm, 
                    global_prediction)
```



```{r}
template_medium <- raster(here("data", "templates", "template_medium.tif")) %>% 
  rasterToPolygons(na.rm = FALSE) %>% 
  st_as_sf(crs = "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs")

map_sf <- map_df %>% 
  st_as_sf(coords = c("x", "y"),
           crs ="+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs") 

map_sf_poly <- st_join(template_medium, 
                       map_sf, 
                       join = st_covers)
```

```{r}
ggplot() +
  geom_sf(data = map_sf_poly %>% filter(estimate < 0.65), aes(fill = estimate, color = estimate)) +
  scale_fill_gradientn(colors = pal) +
  scale_color_gradientn(colors = pal, guide = NULL) + 
  geom_sf(data = world_base_map, fill = "transparent") +
  labs(fill = "GDE") +
  theme_minimal()
```



```{r}
plot(model_rf, type = "spatial-residual", link = TRUE) +
  geom_point(size = 3)
```

```{r}
plot(model_rf, type = "residual-vs-fitted")
```
```{r}
plot(model_rf, type = "prediction", link = FALSE) +
  viridis::scale_colour_viridis() +
  geom_point(size = 3)
```

```{r}
tidy(model_rf, conf.int = TRUE, conf.method = "HPDinterval")
```

###### GDE Freeze line

```{r}
model_t <- glmmfields(
  hill_1 ~
    min_temp,
  data = temp_trend_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 25,
  iter = 3000,
  chains = 2,
  save_log_lik = TRUE,
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 100,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.5),
  prior_sigma = half_t(100, 0, 1),
  prior_gp_theta = half_t(100, 0, 5),
  prior_gp_sigma = half_t(100, 0, 1),
  control = list(adapt_delta = 0.9999,
                 max_treedepth = 15),
  seed = 5369
)
```
```{r}
model_t
```

```{r}
post_draws <- posterior_predict(model_t, iter = 500) %>% 
  t()
colnames(post_draws) <- paste0("draw_", 1:500)

post_df <- post_draws %>% 
  as_tibble() %>% 
  mutate(observed = model_t$data$hill_1) %>% 
  pivot_longer(cols = everything(),
               names_to = "draw",
               values_to = "hill_1_post") %>% 
  mutate(post_samples = if_else(draw == "observed", "observed", "posterior"))

ggplot(data = post_df, aes(x = hill_1_post, 
                           group = draw,
                           color = post_samples)) +
  stat_density(geom = "line", alpha = 0.6, position = "identity") +
  scale_color_manual(values = c("darkgreen", "darkgrey")) +
  theme_minimal()

```



```{r}

t_post_sum <- tidy(model_t, conf.int = TRUE, conf.method = "HPDinterval") 
  
t_post_beta <- t_post_sum %>% 
  filter(str_detect(term, "B"),
         !str_detect(term, "[1]")) %>% 
  mutate(significance = if_else(
    conf.low <= 0 & conf.high >= 0, 0.6, 1.0
  ))

ggplot(data = t_post_beta, aes(x = term, alpha = significance)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  scale_alpha(guide = "none") +
  
  coord_flip() +
  theme_minimal()
```


###### GDM Full Model

```{r}
model_gdm_full <- glmmfields(
  sqrt_pi ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    ghh_medium_std_dev +
    human_medium_gHM +
    stability_precip_medium +
    stability_temp_medium,
  data = df_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 25,
  iter = 3000,
  save_log_lik = TRUE,
  chains = 4,
  prior_intercept = student_t(
    df = 100,
    location = 0,
    scale = 0.1
  ),
  # betas are going to be very small too (definitely under 0.01), because the predictors are normalized and centered, and the response doesn't go above 0.1). So a sigma of 0.01 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.01),
  prior_sigma = half_t(100, 0, 1),
  prior_gp_theta = half_t(100, 0, 5),
  prior_gp_sigma = half_t(100, 0, 1),
  control = list(adapt_delta = 0.9999,
                 max_treedepth = 15),
  seed = 9142
)

```
```{r}

full_gdm_post_sum <- tidy(model_gdm_full, conf.int = TRUE, conf.method = "HPDinterval") 
  
full_gdm_post_beta <- full_gdm_post_sum %>% 
  filter(str_detect(term, "B"),
         !str_detect(term, "[1]")) %>% 
  mutate(significance = if_else(
    conf.low <= 0 & conf.high >= 0, 0.6, 1.0
  ))

ggplot(data = full_gdm_post_beta, aes(x = term, alpha = significance)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  scale_alpha(guide = "none") +
  
  coord_flip() +
  theme_minimal()
```

Posterior density. 100 draws from the posterior prediction distribution overlaid with the observed sqrt_pi.
```{r}
post_draws_gdm_full <- posterior_predict(model_gdm_full, iter = 100) %>% 
  t()
colnames(post_draws_gdm_full) <- paste0("draw_", 1:100)

post_df_gdm_full <- post_draws_gdm_full %>% 
  as_tibble() %>% 
  mutate(observed = model_gdm_full$data$sqrt_pi) %>% 
  pivot_longer(cols = everything(),
               names_to = "draw",
               values_to = "sqrt_pi_post") %>% 
  mutate(post_samples = if_else(draw == "observed", "observed", "posterior"))

ggplot(data = post_df_gdm_full, aes(x = sqrt_pi_post, 
                           group = draw,
                           color = post_samples)) +
  stat_density(geom = "line", alpha = 0.6, position = "identity") +
  scale_color_manual(values = c("darkgreen", "darkgrey")) +
  theme_minimal()

```

```{r}
post_gdm_full <- rstan::extract(model_gdm_full$model, permute = TRUE)

post_beta_gdm_full <- post_gdm_full$B

# colnames(post_beta_gdm_full) <- c(
#   "intercept",
#   "Prec. Wettest Month",
#     "Prec. Driest Month",
#     "Prec. Seasonality",
#     "Temp. Diurnal Range",
#     "Max. Temp Warmest Month",
#     "Precip. Stability",
#     "Temp. Stability"
# )
post_beta_df_gdm_full <- as_tibble(post_beta_gdm_full)

post_beta_sample_gdm_full <- sample_n(post_beta_df_gdm_full, 1000)


ggplot(data = model_gdm_full$data, 
             aes(x = current_medium_bio_5, y = sqrt_pi)) +
  geom_point() +
  geom_abline(data = post_beta_sample_gdm_full, 
              aes(intercept = V1, 
                  slope = V6), 
              alpha = 0.05,
              color = "darkgrey") +
  geom_abline(intercept = full_gdm_post_sum %>% 
                filter(term == "B[1]") %>% 
                pull(estimate),
              slope = full_gdm_post_sum %>% 
                filter(term == "B[6]") %>% 
                pull(estimate),
              color = "darkgreen") +
  labs(x = "Max. Temp. of Warmest Month",
       y = expression(sqrt(GDM))) +
  theme_minimal()

```

Global map of genetic diversity
```{r}
predictors_gdm_full <- colnames(model_gdm_full$X)[-1]
global_predictors_gdm_full <- as.data.frame(rasters_full_medium,
                                   xy = TRUE,
                                   centroids = TRUE,
                                   na.rm = TRUE) %>% 
  select(predictors_gdm_full, x, y)

global_predictors_gdm_full <- global_predictors_gdm_full %>% 
  mutate(cell = rownames(global_predictors_gdm_full),
         lon_scaled = x * 0.000001,
         lat_scaled = y * 0.000001)


normalize <- function(x) {
  a <- x - mean(x)
  b <- sd(x)
  c <- a / b
  return(c)
}

predictors_gdm_full_norm <- global_predictors_gdm_full %>% 
  mutate_at(predictors_gdm_full, normalize) 

global_prediction_gdm_full <- predict(model_gdm_full, 
                             predictors_gdm_full_norm, 
                             estimate_method = "median",
                             conf_level = 0.95,
                             interval = "confidence")

map_df_gdm_full <- bind_cols(global_predictors_gdm_full, 
                    global_prediction_gdm_full)
```



```{r}
map_sf_gdm_full <- map_df_gdm_full %>% 
  st_as_sf(coords = c("x", "y"),
           crs ="+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs")

map_sf_poly_gdm_full <- st_join(template_medium, 
                       map_sf_gdm_full, 
                       join = st_covers)
```

```{r}
ggplot() +
  geom_sf(data = map_sf_poly_gdm_full, aes(fill = estimate, color = estimate)) +
  scale_fill_gradientn(colors = pal, na.value = "transparent") +
  scale_color_gradientn(colors = pal, na.value = "transparent", guide = NULL) +
  geom_sf(data = world_base_map, fill = "transparent") +
  labs(fill = expression(sqrt(GDM))) +
  theme_minimal()
```


```{r}
ggplot() +
   geom_sf(data = df_150, aes(fill = stability_temp_medium, color = stability_temp_medium)) + 
   scale_fill_gradientn(colors = pal, na.value = "transparent") +
   scale_color_gradientn(colors = pal, na.value = "transparent", guide = NULL) +
   geom_sf(data = world_base_map, fill = "transparent") +
   labs(fill = "Temp. Stability") +
   theme_minimal()
```

New stability
```{r}
template_medium_rast <- raster(here("data", "templates", "template_medium.tif"))

temp_trend <- raster("/Users/connorfrench/Downloads/data/past_climate/GlobalExtreme_tsTrendExt.tif") %>% 
  projectRaster(template_medium_rast) %>% 
  rasterToPolygons() %>% 
  st_as_sf()

# take the median of all overlapping cells with each of the medium resolution cells
temp_trend_df <- st_join(df_150, 
                         temp_trend,
                         largest = TRUE) 

ggplot() +
  geom_sf(data = temp_trend_df, aes(fill = GlobalExtreme_tsTrendExt,
                                 color = GlobalExtreme_tsTrendExt)) +
  scale_fill_gradientn(colors = pal) +
  scale_color_gradientn(colors = pal) +
  theme_minimal()

```

```{r}
ggplot(data = temp_trend_df, aes(x = GlobalExtreme_tsTrendExt, y = hill_1)) +
  geom_point() +
  geom_smooth(method = "lm")
```

Precipitation stability
```{r}
precip_trend <- raster("/Users/connorfrench/Downloads/data/past_climate/GlobalExtreme_prVarExt.tif") %>% 
  projectRaster(template_medium_rast) %>% 
  rasterToPolygons() %>% 
  st_as_sf()

# take the median of all overlapping cells with each of the medium resolution cells
precip_trend_df <- st_join(df_150, 
                         precip_trend,
                         largest = TRUE)

ggplot() +
  geom_sf(data = precip_trend_df, aes(fill = GlobalExtreme_prVarExt,
                                 color = GlobalExtreme_prVarExt)) +
  scale_fill_gradientn(colors = pal) +
  scale_color_gradientn(colors = pal) +
  theme_minimal()
```

```{r}
ggplot(data = precip_trend_df, aes(x = GlobalExtreme_prVarExt, y = hill_1)) +
  geom_point() +
  geom_smooth(method = "lm")
```

```{r}

new_vars <- colnames(new_stab_spatial)

new_vars <- new_vars[str_detect(new_vars, "current|temp_PC|precip_PC|dhi|_trend|_var|stability")]

plot_predictors <- function(predictor){
  ggplot(data = new_stab_spatial, aes_string(x = predictor, y = "hill_1", color = "min_temp")) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_color_viridis_d() +
  theme_minimal()
}

hill_1_plots <- map(new_vars, plot_predictors)

  
```


```{r}
hill_1_plots[[1]] + hill_1_plots[[2]] + hill_1_plots[[3]] + hill_1_plots[[4]]
```



```{r}
hill_1_plots[[5]] + hill_1_plots[[6]] + hill_1_plots[[7]] + hill_1_plots[[8]]
```


```{r}
hill_1_plots[[9]] + hill_1_plots[[10]] + hill_1_plots[[11]] + hill_1_plots[[12]]
```


```{r}
hill_1_plots[[13]] + hill_1_plots[[14]] + hill_1_plots[[15]] + hill_1_plots[[16]]
```

```{r}
hill_1_plots[[17]] + hill_1_plots[[18]] + hill_1_plots[[19]] + hill_1_plots[[20]]
```


```{r}
hill_1_plots[[21]] + hill_1_plots[[22]] + hill_1_plots[[23]] + hill_1_plots[[24]]
```

```{r}
hill_1_plots[[21]] + hill_1_plots[[22]] + hill_1_plots[[23]] + hill_1_plots[[24]]
```

```{r}
hill_1_plots[[25]] + hill_1_plots[[26]] + hill_1_plots[[27]] + hill_1_plots[[28]]
```


```{r}
hill_1_plots[[29]] + hill_1_plots[[30]] + hill_1_plots[[31]] + hill_1_plots[[32]]
```

```{r}
hill_1_plots[[25]] / hill_1_plots[[26]]
```

```{r}
summary(lm(hill_1 ~ temp_PC2 * min_temp + temp_trend * min_temp, data = new_stab_spatial))
```

```{r}
summary(lm(hill_1 ~ current_medium_bio_5 * min_temp + stability_temp_medium * min_temp, data = new_stab_spatial))
```


##### Model comparison

```{r}
loo_cs <- loo(model_cs)
loo_t <- loo(model_t)


loo::loo_compare(loo_cs, loo_t)
```

